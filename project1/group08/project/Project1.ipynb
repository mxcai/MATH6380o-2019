{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and variable definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from skimage import io, transform\n",
    "from skimage.color import rgba2rgb\n",
    "from collections import OrderedDict\n",
    "from kymatio import Scattering2D\n",
    "import kymatio.datasets as scattering_datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "TRAIN = 'train'\n",
    "VAL = 'val'\n",
    "PRED = 'pred'\n",
    "\n",
    "use_cuda = True\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "print ('Using GPU:', use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load images and labels, preprocessing, and convert to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get painting labels\n",
    "labels = {}\n",
    "\n",
    "with open('Project1-Raphael/label.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter='.')\n",
    "    for row in csv_reader:\n",
    "        labels[row[0]] = row[1].strip()\n",
    "\n",
    "\n",
    "# load images of painting and construct into dataframe\n",
    "image_extensions = ['.TIF', '.tif', '.tiff', '.jpg']\n",
    "data_set = []\n",
    "\n",
    "for file in os.listdir('Project1-Raphael'):\n",
    "    extension = os.path.splitext(file)[1]\n",
    "    \n",
    "    if extension in image_extensions:\n",
    "        \n",
    "        filename = os.path.splitext(file)[0]\n",
    "        image = io.imread(os.path.join('Project1-Raphael', file))\n",
    "        if image.shape[2] == 4:\n",
    "            image = rgba2rgb(image)\n",
    "        image = np.array(image)\n",
    "        if image.max() <= 1:\n",
    "            image = (image * 255).astype(np.uint8)\n",
    "        if image.max() > 255:\n",
    "            image = (image / 255).astype(np.uint8)\n",
    "        else:\n",
    "            image_id = filename.split(' ')[0].replace('.', '')\n",
    "            label = labels[image_id]\n",
    "\n",
    "            if label == 'Raphael':\n",
    "                data_set.append({'Image': image, 'Disputed': 0, 'Raphael': 1, 'ID':image_id})\n",
    "            elif label == 'Not Raphael':\n",
    "                data_set.append({'Image': image, 'Disputed': 0, 'Raphael': 0, 'ID':image_id})\n",
    "            else:\n",
    "                data_set.append({'Image': image, 'Disputed': 1, 'Raphael': -1, 'ID':image_id})\n",
    "\n",
    "df = pd.DataFrame(data_set)\n",
    "training_df = df[df['Disputed'] == 0]\n",
    "validation_df = df[df['Disputed'] == 0]\n",
    "prediction_df = df[df['Disputed'] == 1]\n",
    "dataframes = {'train': training_df, 'val': training_df, 'pred':prediction_df}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaphaelPaintingsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (DataFrame): A dataframe containing painting image and painted by Raphael label.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.image = df['Image'].values\n",
    "        self.raphael = df['Raphael'].values\n",
    "        self.data_id = df['ID'].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raphael)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        image = Image.fromarray(self.image[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor(self.raphael[idx], dtype=torch.int64)\n",
    "        data_id = self.data_id[idx]\n",
    "        \n",
    "        sample = {'image': image, 'label': label, 'id': data_id}\n",
    "        return sample\n",
    "    \n",
    "\n",
    "data_transforms = {\n",
    "    TRAIN: transforms.Compose([\n",
    "        transforms.Resize(512),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ]),\n",
    "    VAL: transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ]),\n",
    "    PRED: transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "}\n",
    "\n",
    "datasets = {\n",
    "    x: RaphaelPaintingsDataset(dataframes[x], data_transforms[x])\n",
    "    for x in [TRAIN, VAL, PRED]\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(datasets[x], batch_size=12, shuffle=True, num_workers=2)\n",
    "    for x in [TRAIN, VAL, PRED]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define train and validate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, scattering=None):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_accuracy = 0\n",
    "    \n",
    "    for samples in train_loader:\n",
    "        images, labels = samples['image'], samples['label']\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scattering:\n",
    "            output = model(scattering(images))\n",
    "        else:\n",
    "            output = model.forward(images)\n",
    "        \n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        ps = torch.exp(output)\n",
    "        equality = (labels.data == ps.max(dim=1)[1])\n",
    "        train_accuracy += equality.type(torch.FloatTensor).mean()\n",
    "        \n",
    "    return train_loss, train_accuracy\n",
    "        \n",
    "def validate(model, device, validate_loader, scattering=None):\n",
    "    model.eval()\n",
    "    validate_loss = 0\n",
    "    validate_accuracy = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for samples in validate_loader:\n",
    "            images, labels = samples['image'], samples['label']\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            if scattering:\n",
    "                output = model(scattering(images))\n",
    "            else:\n",
    "                output = model.forward(images)\n",
    "            \n",
    "            validate_loss += criterion(output, labels).item()\n",
    "            \n",
    "            ps = torch.exp(output)\n",
    "            equality = (labels.data == ps.max(dim=1)[1])\n",
    "            validate_accuracy += equality.type(torch.FloatTensor).mean()\n",
    "            \n",
    "    return validate_loss, validate_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invariant scattering networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100..  Train (Loss: 125.1635 Accuracy: 0.875 )  Validate (Loss: 32.986 Accuracy: 0.73 )\n",
      "Epoch: 20/100..  Train (Loss: 2.6660 Accuracy: 0.958 )  Validate (Loss: 15.248 Accuracy: 0.75 )\n",
      "Epoch: 30/100..  Train (Loss: 107.9344 Accuracy: 0.854 )  Validate (Loss: 20.867 Accuracy: 0.79 )\n",
      "Epoch: 40/100..  Train (Loss: 27.5427 Accuracy: 0.854 )  Validate (Loss: 0.000 Accuracy: 1.00 )\n",
      "Epoch: 50/100..  Train (Loss: 184.1241 Accuracy: 0.792 )  Validate (Loss: 14.524 Accuracy: 0.92 )\n",
      "Epoch: 60/100..  Train (Loss: 109.3516 Accuracy: 0.667 )  Validate (Loss: 0.000 Accuracy: 1.00 )\n",
      "Epoch: 70/100..  Train (Loss: 201.6019 Accuracy: 0.833 )  Validate (Loss: 5.525 Accuracy: 0.96 )\n",
      "Epoch: 80/100..  Train (Loss: 13.1229 Accuracy: 0.938 )  Validate (Loss: 2.105 Accuracy: 0.96 )\n",
      "Epoch: 90/100..  Train (Loss: 28.8404 Accuracy: 0.896 )  Validate (Loss: 0.000 Accuracy: 1.00 )\n",
      "Epoch: 100/100..  Train (Loss: 3.6168 Accuracy: 0.958 )  Validate (Loss: 0.000 Accuracy: 1.00 )\n"
     ]
    }
   ],
   "source": [
    "scattering = Scattering2D(J=2, shape=(224, 224))\n",
    "K = 81*3\n",
    "if use_cuda:\n",
    "    scattering = scattering.cuda()\n",
    "\n",
    "\n",
    "class View(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(View, self).__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(-1,*self.shape)\n",
    "\n",
    "\n",
    "ScatterMLP = nn.Sequential(View(K, 56, 56),\n",
    "                           nn.BatchNorm2d(K),\n",
    "                           View(K*56*56),\n",
    "                           nn.Linear(K*56*56, 512),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Linear(512, 256),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Linear(256, 2),\n",
    "                           nn.LogSoftmax(dim=1))\n",
    "ScatterMLP.to(device)\n",
    "\n",
    "for m in ScatterMLP.modules():\n",
    "    if isinstance(m, nn.Linear):\n",
    "        m.weight.data.normal_(0, 2./math.sqrt(m.in_features))\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "\n",
    "# model training\n",
    "epochs = 100\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(ScatterMLP.parameters(), lr=0.001)\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    train_loss, train_accuracy = train(ScatterMLP, device, dataloaders[TRAIN], optimizer, scattering)\n",
    "    validate_loss, validate_accuracy = validate(ScatterMLP, device, dataloaders[VAL], scattering) \n",
    "    \n",
    "    if (e+1) % 10 == 0:\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Train (Loss: {:.4f}\".format(train_loss/len(dataloaders[TRAIN])),\n",
    "              \"Accuracy: {:.3f}\".format(train_accuracy/len(dataloaders[TRAIN])),\n",
    "              \")  Validate (Loss: {:.3f}\".format(validate_loss/len(dataloaders[VAL])),\n",
    "              \"Accuracy: {:.2f}\".format(validate_accuracy/len(dataloaders[VAL])),\n",
    "              \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning using pretrained vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20..  Train (Loss: 4.6763 Accuracy: 0.521 )  Validate (Loss: 2.200 Accuracy: 0.56 )\n",
      "Epoch: 2/20..  Train (Loss: 1.5558 Accuracy: 0.542 )  Validate (Loss: 1.006 Accuracy: 0.77 )\n",
      "Epoch: 3/20..  Train (Loss: 0.6594 Accuracy: 0.708 )  Validate (Loss: 0.278 Accuracy: 0.81 )\n",
      "Epoch: 4/20..  Train (Loss: 0.6684 Accuracy: 0.833 )  Validate (Loss: 1.148 Accuracy: 0.73 )\n",
      "Epoch: 5/20..  Train (Loss: 0.9732 Accuracy: 0.875 )  Validate (Loss: 0.398 Accuracy: 0.88 )\n",
      "Epoch: 6/20..  Train (Loss: 0.5604 Accuracy: 0.875 )  Validate (Loss: 0.032 Accuracy: 1.00 )\n",
      "Epoch: 7/20..  Train (Loss: 0.3123 Accuracy: 0.896 )  Validate (Loss: 0.112 Accuracy: 0.96 )\n",
      "Epoch: 8/20..  Train (Loss: 0.1642 Accuracy: 0.854 )  Validate (Loss: 0.076 Accuracy: 0.94 )\n",
      "Epoch: 9/20..  Train (Loss: 0.1951 Accuracy: 0.958 )  Validate (Loss: 0.051 Accuracy: 1.00 )\n",
      "Epoch: 10/20..  Train (Loss: 0.0343 Accuracy: 1.000 )  Validate (Loss: 0.111 Accuracy: 0.94 )\n",
      "Epoch: 11/20..  Train (Loss: 0.2670 Accuracy: 0.958 )  Validate (Loss: 0.082 Accuracy: 0.94 )\n",
      "Epoch: 12/20..  Train (Loss: 0.7132 Accuracy: 0.896 )  Validate (Loss: 0.007 Accuracy: 1.00 )\n",
      "Epoch: 13/20..  Train (Loss: 0.0670 Accuracy: 1.000 )  Validate (Loss: 0.020 Accuracy: 1.00 )\n",
      "Epoch: 14/20..  Train (Loss: 0.0448 Accuracy: 1.000 )  Validate (Loss: 0.035 Accuracy: 1.00 )\n",
      "Epoch: 15/20..  Train (Loss: 0.2027 Accuracy: 0.938 )  Validate (Loss: 0.053 Accuracy: 0.96 )\n",
      "Epoch: 16/20..  Train (Loss: 0.0151 Accuracy: 1.000 )  Validate (Loss: 0.118 Accuracy: 0.90 )\n",
      "Epoch: 17/20..  Train (Loss: 0.3226 Accuracy: 0.896 )  Validate (Loss: 0.063 Accuracy: 0.94 )\n",
      "Epoch: 18/20..  Train (Loss: 0.2336 Accuracy: 0.896 )  Validate (Loss: 0.038 Accuracy: 0.96 )\n",
      "Epoch: 19/20..  Train (Loss: 0.2197 Accuracy: 0.958 )  Validate (Loss: 0.016 Accuracy: 1.00 )\n",
      "Epoch: 20/20..  Train (Loss: 0.4057 Accuracy: 0.875 )  Validate (Loss: 0.005 Accuracy: 1.00 )\n"
     ]
    }
   ],
   "source": [
    "# model definition\n",
    "vgg19 = models.vgg19(pretrained=True)\n",
    "\n",
    "for param in vgg19.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "classifier = nn.Sequential(OrderedDict([('fc1', nn.Linear(25088, 1024)),\n",
    "                                        ('relu1', nn.ReLU()),\n",
    "                                        ('fc2', nn.Linear(1024,2)),\n",
    "                                        ('output', nn.LogSoftmax(dim=1))\n",
    "                                       ]))\n",
    "\n",
    "vgg19.classifier = classifier\n",
    "vgg19.to(device)\n",
    "\n",
    "\n",
    "# model training\n",
    "epochs = 20\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(vgg19.classifier.parameters(), lr=0.001)\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    train_loss, train_accuracy = train(vgg19, device, dataloaders[TRAIN], optimizer)\n",
    "    validate_loss, validate_accuracy = validate(vgg19, device, dataloaders[VAL]) \n",
    "            \n",
    "    print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "          \"Train (Loss: {:.4f}\".format(train_loss/len(dataloaders[TRAIN])),\n",
    "          \"Accuracy: {:.3f}\".format(train_accuracy/len(dataloaders[TRAIN])),\n",
    "          \")  Validate (Loss: {:.3f}\".format(validate_loss/len(dataloaders[VAL])),\n",
    "          \"Accuracy: {:.2f}\".format(validate_accuracy/len(dataloaders[VAL])),\n",
    "          \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction for ScatterNet and vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction by scattering MLP model:\n",
      " {'1': 0, '10': 1, '20': 1, '23': 0, '25': 0, '26': 1, '7': 1}\n",
      "Prediction by vgg16 model:\n",
      " {'1': 0, '10': 0, '20': 0, '23': 0, '25': 0, '26': 0, '7': 0}\n"
     ]
    }
   ],
   "source": [
    "def predict(model, device, predict_loader, scattering=None):\n",
    "    model.eval()\n",
    "    predict = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for samples in predict_loader:\n",
    "            images = samples['image']\n",
    "            images = images.to(device)\n",
    "            \n",
    "            if scattering:\n",
    "                output = model(scattering(images))\n",
    "            else:\n",
    "                output = model.forward(images)\n",
    "            \n",
    "            ps = torch.exp(output).detach().cpu().numpy().astype(np.uint8)\n",
    "            data_id = samples['id']\n",
    "            \n",
    "            for i in range(len(data_id)):\n",
    "                predict[data_id[i]] = ps[i][0]\n",
    "    \n",
    "    predict = dict(sorted(predict.items()))\n",
    "    return predict\n",
    "\n",
    "\n",
    "scatter_net_predict = predict(ScatterMLP, device, dataloaders[PRED], scattering)\n",
    "print (\"Prediction by scattering MLP model:\\n\", scatter_net_predict)\n",
    "\n",
    "vgg19_predict = predict(vgg19, device, dataloaders[PRED])\n",
    "print (\"Prediction by vgg16 model:\\n\", vgg19_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 1\n",
    "\n",
    "Comparing the first two feature extraction approches, the deep learning model perform much better than the scattering net model. Using transfoer learning on pretrained vgg19, we see a fast and study decrease in both traing and validation accuracy, acheaving 100% in a few epoch. Note the validation accuracy is higher than traing accuracy because with random crop and random flip, the traing set is much harder than the validation and prediction set. This is data augmentation to improve model training. In this part, we can say the deep learning model is better and should give a more accurate prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing for Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def image_resize(image):\n",
    "    image = Image.fromarray(image)\n",
    "    image = image.resize((224, 224))\n",
    "    return np.array(image)\n",
    "\n",
    "df['Image'] = df['Image'].map(lambda image: image_resize(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical unsupervised learning: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA explained in 1st dimension is 0.241330\n",
      "PCA explained in 2nd dimension is 0.135560\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN prediction accuracy on the labeled set is: 0.85\n",
      "2 components PCA with 3 neighbors KNN prediction:\n",
      " {'1': 1, '10': 0, '20': 1, '23': 0, '25': 0, '26': 0, '7': 0}\n"
     ]
    }
   ],
   "source": [
    "# fit 2 components PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "image_set = np.array(df['Image'].values.tolist()).reshape((28,-1))\n",
    "pca_image_set = pca.fit_transform(image_set)\n",
    "print (\"PCA explained in 1st dimension is {:f}\"\"\\nPCA explained in 2nd dimension is {:f}\"\n",
    "       .format(*pca.explained_variance_ratio_))\n",
    "\n",
    "\n",
    "# plot data\n",
    "D_1 = [pca_image_set[i, 0] for i in df.loc[df['Raphael']==1].index]\n",
    "D_2 = [pca_image_set[i, 1] for i in df.loc[df['Raphael']==1].index]\n",
    "raphael_scaplt = plt.scatter(x=D_1, y=D_2, color='b', label='Raphael')\n",
    "\n",
    "D_1 = [pca_image_set[i, 0] for i in df.loc[df['Raphael']==0].index]\n",
    "D_2 = [pca_image_set[i, 1] for i in df.loc[df['Raphael']==0].index]\n",
    "not_raphael_sacplt = plt.scatter(x=D_1, y=D_2, color='r', label='not Raphael')\n",
    "\n",
    "D_1 = [pca_image_set[i, 0] for i in df.loc[df['Disputed']==1].index]\n",
    "D_2 = [pca_image_set[i, 1] for i in df.loc[df['Disputed']==1].index]\n",
    "disputed_scaplt = plt.scatter(x=D_1, y=D_2, color='y', label='disputed')\n",
    "\n",
    "plt.title('First 2 dimension PCA')\n",
    "plt.legend(handles=[raphael_scaplt, not_raphael_sacplt, disputed_scaplt])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# knn model\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "labeled_pca_image_set = [pca_image_set[i] for i in df.loc[df['Disputed']==0].index]\n",
    "labeled_raphael_set = [df['Raphael'][i] for i in df.loc[df['Disputed']==0].index]\n",
    "neigh.fit(labeled_pca_image_set, labeled_raphael_set) \n",
    "\n",
    "predicts = neigh.predict(labeled_pca_image_set)\n",
    "incorrect_prediction_count = sum(abs(predicts - labeled_raphael_set))\n",
    "accuracy = 1 - incorrect_prediction_count/len(labeled_pca_image_set)                   \n",
    "print ('KNN prediction accuracy on the labeled set is:', accuracy)\n",
    "\n",
    "disputed_pca_image_set = [pca_image_set[i] for i in df.loc[df['Disputed']==1].index]\n",
    "disputed_id_set = [df['ID'][i] for i in df.loc[df['Disputed']==1].index]\n",
    "predicts = neigh.predict(disputed_pca_image_set)\n",
    "knn_predictions = {}\n",
    "for data_id, prediction in zip(disputed_id_set, predicts):\n",
    "    knn_predictions[data_id] = prediction\n",
    "knn_predictions = dict(sorted(knn_predictions.items()))\n",
    "print ('2 components PCA with 3 neighbors KNN prediction:\\n', knn_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional supervised learning: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM prediction accuracy on the labeled set is: 1.0\n",
      "SVM prediction:\n",
      " {'1': 1, '10': 1, '20': 1, '23': 1, '25': 1, '26': 1, '7': 1}\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(gamma='auto')\n",
    "labeled_pca_image_set = [pca_image_set[i] for i in df.loc[df['Disputed']==0].index]\n",
    "labeled_raphael_set = [df['Raphael'][i] for i in df.loc[df['Disputed']==0].index]\n",
    "clf.fit(labeled_pca_image_set, labeled_raphael_set) \n",
    "\n",
    "\n",
    "predicts = clf.predict(labeled_pca_image_set)\n",
    "incorrect_prediction_count = sum(abs(predicts - labeled_raphael_set))\n",
    "accuracy = 1 - incorrect_prediction_count/len(labeled_pca_image_set)                   \n",
    "print ('SVM prediction accuracy on the labeled set is:', accuracy)\n",
    "\n",
    "disputed_pca_image_set = [pca_image_set[i] for i in df.loc[df['Disputed']==1].index]\n",
    "disputed_id_set = [df['ID'][i] for i in df.loc[df['Disputed']==1].index]\n",
    "predicts = clf.predict(disputed_pca_image_set)\n",
    "knn_predictions = {}\n",
    "for data_id, prediction in zip(disputed_id_set, predicts):\n",
    "    knn_predictions[data_id] = prediction\n",
    "knn_predictions = dict(sorted(knn_predictions.items()))\n",
    "print ('SVM prediction:\\n', knn_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 2\n",
    "\n",
    "Comparing traditional approches, we see the SVM model is doing better in the training set. This is to be expected as the 2 compoments PCA model only in total captures explained variance of less than 30%, which is not a high number for a PCA. This also represent the difference in the images may be trivial and hard to distinguish. For this SVM model, it gives an all by Raphael prediction for the disputed images. This result is being different from what vgg19 model predicts even though both have a 100% accuracy on the training set. As this being a very small dataset, with only 21 labeled datas, leave-one-out cross validation should gives us a better understanding on the actual performance of the model. However, this is a little hard to run on a PC and will not be implemented. In general, because of how data augmentation in implemented in the vgg19 model, it is much less likely to overfit than the simple SVM model. So I will say the best and most reasonable prediction in the four implemeted models is the vgg19 transfer learning model, though it does produce an all not by Raphael prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
